from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from typing import List, Optional
import os
import tempfile
import json
from pathlib import Path

try:
    from whisper_handler import WhisperHandler
    WHISPER_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Whisper not available: {e}")
    print("Server will run in TEST mode without speech recognition")
    WHISPER_AVAILABLE = False

from skill_matcher import SkillMatcher

# Global state
whisper_handler = None
skill_matcher = None
current_skills = []
download_progress = {}  # {model_size: {"status": "downloading", "progress": 0-100}}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan manager (modern FastAPI pattern)
    Replaces deprecated @app.on_event("startup") and @app.on_event("shutdown")
    """
    # Startup
    global whisper_handler
    print("=" * 60)
    print("ğŸ¤ Voice Recognition Server - Starting")
    print("=" * 60)

    if WHISPER_AVAILABLE:
        print("[*] Initializing Whisper model (base)...")
        try:
            whisper_handler = WhisperHandler(model_size="base")
            print("[âœ“] Whisper model loaded successfully!")
        except Exception as e:
            print(f"[!] Failed to load Whisper model: {e}")
            print("[*] Server will run in TEST mode")
    else:
        print("[*] Server running in TEST mode (Whisper not available)")

    print("[âœ“] Server ready on http://0.0.0.0:8000")
    print("=" * 60)

    yield  # Server is running

    # Shutdown
    print("\n[*] Shutting down server...")
    print("[âœ“] Cleanup complete")

# FastAPI app with lifespan
app = FastAPI(
    title="Voice Recognition Skill Matcher",
    description="AI-powered voice recognition server for Lost Spells game",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware (allow Unity to connect)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Production: restrict to specific domains
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "message": "Voice Recognition Server",
        "current_skills": current_skills
    }

@app.post("/set-skills")
async def set_skills(skills: str = Form(...)):
    """
    ìŠ¤í‚¬ ëª©ë¡ ì„¤ì •

    Args:
        skills: ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ìŠ¤í‚¬ ëª©ë¡ (ì˜ˆ: "ê°€,ë‚˜,ë‹¤,ë¼,ë§ˆ")
    """
    global skill_matcher, current_skills

    try:
        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ìŠ¤í‚¬ íŒŒì‹±
        skill_list = [s.strip() for s in skills.split(",") if s.strip()]

        if not skill_list:
            raise HTTPException(status_code=400, detail="No skills provided")

        current_skills = skill_list
        skill_matcher = SkillMatcher(skill_list)

        return {
            "status": "success",
            "message": f"Skills set successfully",
            "skills": current_skills
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error setting skills: {str(e)}")

@app.post("/recognize")
async def recognize_skill(
    audio: UploadFile = File(...),
    skills: Optional[str] = Form(None),
    language: Optional[str] = Form("ko")
):
    """
    ìŒì„± íŒŒì¼ì—ì„œ ìŠ¤í‚¬ ì¸ì‹

    Args:
        audio: ìŒì„± íŒŒì¼ (WAV, MP3 ë“±)
        skills: (ì„ íƒ) ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ìŠ¤í‚¬ ëª©ë¡
        language: (ì„ íƒ) ì–¸ì–´ ì½”ë“œ (ko, en ë“±)

    Returns:
        {
            "recognized_text": "ì¸ì‹ëœ í…ìŠ¤íŠ¸",
            "processing_time": 0.5,
            "skill_scores": {"ê°€": 0.95, "ë‚˜": 0.10, ...}
        }
    """
    global whisper_handler, skill_matcher, current_skills

    if not WHISPER_AVAILABLE:
        # TEST MODE: Whisper ì—†ì´ ë”ë¯¸ ì‘ë‹µ ë°˜í™˜
        if not current_skills:
            current_skills = ["ê°€", "ë‚˜", "ë‹¤", "ë¼", "ë§ˆ"]
            skill_matcher = SkillMatcher(current_skills)

        return {
            "status": "success",
            "recognized_text": "í…ŒìŠ¤íŠ¸",
            "processing_time": 0.1,
            "skill_scores": {"ê°€": 0.85, "ë‚˜": 0.12, "ë‹¤": 0.08, "ë¼": 0.15, "ë§ˆ": 0.05},
            "best_match": {"skill": "ê°€", "score": 0.85},
            "note": "TEST MODE - Whisper not available. Install Python 3.13 and faster-whisper for real recognition."
        }

    if whisper_handler is None:
        raise HTTPException(status_code=500, detail="Whisper model not loaded")

    # ìŠ¤í‚¬ì´ ì œê³µë˜ë©´ ì—…ë°ì´íŠ¸
    if skills:
        skill_list = [s.strip() for s in skills.split(",") if s.strip()]
        current_skills = skill_list
        skill_matcher = SkillMatcher(skill_list)

    if not current_skills:
        raise HTTPException(
            status_code=400,
            detail="No skills configured. Use /set-skills first or provide skills parameter"
        )

    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_file = None
    debug_file = None
    try:
        content = await audio.read()

        # ë””ë²„ê¹…ìš©: ë°›ì€ ì˜¤ë””ì˜¤ë¥¼ Server í´ë”ì— ì €ì¥
        debug_file = Path(__file__).parent / "last_recording.wav"
        with open(debug_file, "wb") as f:
            f.write(content)

        # ì„ì‹œ íŒŒì¼ ìƒì„±
        suffix = Path(audio.filename).suffix if audio.filename else ".wav"
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp:
            temp.write(content)
            temp_file = temp.name

        print(f"[Audio] Received {len(content)} bytes")
        print(f"[Audio] Debug file saved to: {debug_file}")

        # Whisperë¡œ ìŒì„± ì¸ì‹ (ì–¸ì–´ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        transcription = whisper_handler.transcribe_audio(temp_file, language=language)

        recognized_text = transcription["text"]
        processing_time = transcription["processing_time"]

        print(f"[Whisper] Recognized ({language}): '{recognized_text}' (took {processing_time:.2f}s)")

        # ìŠ¤í‚¬ ë§¤ì¹­
        skill_scores = skill_matcher.match_skills(recognized_text)
        best_match = skill_matcher.get_best_match(recognized_text)

        print(f"[Matching] Best match: '{best_match[0]}' (score: {best_match[1]:.2f})")
        print(f"[Matching] All scores: {skill_scores}")

        return {
            "status": "success",
            "recognized_text": recognized_text,
            "processing_time": round(processing_time, 2),
            "skill_scores": skill_scores,
            "best_match": {
                "skill": best_match[0],
                "score": best_match[1]
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing audio: {str(e)}")

    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if temp_file and os.path.exists(temp_file):
            os.remove(temp_file)

@app.get("/skills")
async def get_skills():
    """í˜„ì¬ ì„¤ì •ëœ ìŠ¤í‚¬ ëª©ë¡ ì¡°íšŒ"""
    return {
        "skills": current_skills
    }

@app.get("/models")
async def get_models():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë° ìƒíƒœ ì¡°íšŒ

    Returns:
        {
            "current_model": "base",
            "available_models": {
                "tiny": {"name": "Tiny", "description": "...", "size": "~75MB", "downloaded": true},
                ...
            }
        }
    """
    if not WHISPER_AVAILABLE:
        return {
            "status": "error",
            "message": "Whisper not available"
        }

    available_models = WhisperHandler.get_available_models()

    # ê° ëª¨ë¸ì˜ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸
    models_with_status = {}
    for model_id, model_info in available_models.items():
        models_with_status[model_id] = {
            **model_info,
            "downloaded": WhisperHandler.check_model_downloaded(model_id)
        }

    current_model = whisper_handler.current_model_size if whisper_handler else "none"

    return {
        "status": "success",
        "current_model": current_model,
        "available_models": models_with_status
    }

@app.post("/models/select")
async def select_model(model_size: str = Form(...)):
    """
    ëª¨ë¸ ì„ íƒ ë° ë¡œë“œ

    Args:
        model_size: ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large-v3)

    Returns:
        ì„±ê³µ ë˜ëŠ” ì‹¤íŒ¨ ë©”ì‹œì§€
    """
    global whisper_handler

    if not WHISPER_AVAILABLE:
        raise HTTPException(status_code=500, detail="Whisper not available")

    available_models = WhisperHandler.get_available_models()
    if model_size not in available_models:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid model size. Available: {list(available_models.keys())}"
        )

    try:
        if whisper_handler is None:
            whisper_handler = WhisperHandler(model_size=model_size)
        else:
            whisper_handler.change_model(model_size)

        return {
            "status": "success",
            "message": f"Model changed to {model_size}",
            "current_model": model_size
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading model: {str(e)}")

@app.get("/models/{model_size}/status")
async def get_model_status(model_size: str):
    """
    íŠ¹ì • ëª¨ë¸ì˜ ìƒíƒœ í™•ì¸ (ë‹¤ìš´ë¡œë“œ ì—¬ë¶€, ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ )

    Args:
        model_size: ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large-v3)

    Returns:
        {
            "downloaded": true/false,
            "download_progress": 0-100,
            "status": "downloaded"/"not_downloaded"/"downloading"
        }
    """
    global download_progress

    if not WHISPER_AVAILABLE:
        raise HTTPException(status_code=500, detail="Whisper not available")

    available_models = WhisperHandler.get_available_models()
    if model_size not in available_models:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid model size. Available: {list(available_models.keys())}"
        )

    # ë‹¤ìš´ë¡œë“œ ì¤‘ì¸ì§€ í™•ì¸
    if model_size in download_progress:
        return {
            "downloaded": False,
            "download_progress": download_progress[model_size].get("progress", 0),
            "status": "downloading"
        }

    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸
    is_downloaded = WhisperHandler.check_model_downloaded(model_size)

    if is_downloaded:
        return {
            "downloaded": True,
            "download_progress": 100,
            "status": "downloaded"
        }
    else:
        # ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì „ì´ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°
        return {
            "downloaded": False,
            "download_progress": 0,
            "status": "not_downloaded"
        }

def get_model_download_size(model_size: str) -> int:
    """ëª¨ë¸ì˜ ì˜ˆìƒ ë‹¤ìš´ë¡œë“œ í¬ê¸°(ë°”ì´íŠ¸) ë°˜í™˜"""
    # ê° ëª¨ë¸ì˜ ëŒ€ëµì ì¸ í¬ê¸° (ë°”ì´íŠ¸)
    model_sizes = {
        "tiny": 75 * 1024 * 1024,      # 75MB
        "base": 145 * 1024 * 1024,     # 145MB
        "small": 466 * 1024 * 1024,    # 466MB
        "medium": 1536 * 1024 * 1024,  # 1.5GB
        "large-v3": 2969 * 1024 * 1024 # 2.9GB
    }
    return model_sizes.get(model_size, 100 * 1024 * 1024)

def get_current_model_size_on_disk(model_size: str) -> int:
    """í˜„ì¬ ë””ìŠ¤í¬ì— ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ í¬ê¸°(ë°”ì´íŠ¸) ë°˜í™˜"""
    from pathlib import Path

    cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
    model_pattern = f"models--Systran--faster-whisper-{model_size}"

    total_size = 0
    if cache_dir.exists():
        for item in cache_dir.iterdir():
            if model_pattern in item.name:
                # ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ í¬ê¸° í•©ì‚°
                if item.is_dir():
                    for file in item.rglob("*"):
                        if file.is_file():
                            total_size += file.stat().st_size
                elif item.is_file():
                    total_size += item.stat().st_size

    return total_size

async def download_model_background(model_size: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
    global download_progress
    import asyncio
    import threading

    # ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì „ í˜„ì¬ í¬ê¸° ê¸°ë¡
    initial_size = get_current_model_size_on_disk(model_size)
    expected_size = get_model_download_size(model_size)

    print(f"[Download] Initial size for {model_size}: {initial_size} bytes / {expected_size} bytes")

    def download_thread():
        try:
            # ë‹¤ìš´ë¡œë“œ ìƒíƒœ ì´ˆê¸°í™”
            download_progress[model_size] = {
                "status": "downloading",
                "progress": 0,
                "initial_size": initial_size,
                "completed": False
            }

            # ëª¨ë¸ ë¡œë“œ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
            print(f"[Download] Starting download for model: {model_size}")
            temp_handler = WhisperHandler(model_size=model_size)

            # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
            if model_size in download_progress:
                download_progress[model_size]["progress"] = 100
                download_progress[model_size]["completed"] = True
                print(f"[Download] Model {model_size} downloaded successfully")

            # ë‹¤ìš´ë¡œë“œ ìƒíƒœ ì œê±° (ì™„ë£Œë¨)
            import time
            time.sleep(2)  # 2ì´ˆ í›„ ìƒíƒœ ì œê±° (Unityê°€ 100% í‘œì‹œí•  ì‹œê°„ ì œê³µ)
            if model_size in download_progress:
                del download_progress[model_size]

        except Exception as e:
            print(f"[Download] Error downloading model {model_size}: {e}")
            if model_size in download_progress:
                del download_progress[model_size]

    # ì§„í–‰ë¥  ì¶”ì  ìŠ¤ë ˆë“œ
    def progress_thread():
        import time

        while model_size in download_progress:
            # ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìœ¼ë©´ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì¤‘ì§€
            if download_progress.get(model_size, {}).get("completed", False):
                time.sleep(0.5)
                continue

            current_size = get_current_model_size_on_disk(model_size)

            # ë‹¤ìš´ë¡œë“œëœ ì–‘ ê³„ì‚° (í˜„ì¬ í¬ê¸° - ì´ˆê¸° í¬ê¸°)
            downloaded = current_size - initial_size
            remaining = expected_size - initial_size

            if remaining > 0:
                # ì§„í–‰ë¥  = (ë‹¤ìš´ë¡œë“œëœ ì–‘ / ë‚¨ì€ ì–‘) * 100
                progress = min(int((downloaded / remaining) * 100), 99)  # ìµœëŒ€ 99%ê¹Œì§€ë§Œ
            else:
                # ì´ë¯¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œëœ ê²½ìš°
                progress = 99

            if model_size in download_progress and not download_progress[model_size].get("completed", False):
                download_progress[model_size]["progress"] = progress
                print(f"[Download] Progress for {model_size}: {progress}% (downloaded: {downloaded}/{remaining} bytes)")

            time.sleep(0.5)  # 0.5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸

    # ë‘ ìŠ¤ë ˆë“œ ì‹œì‘
    download_t = threading.Thread(target=download_thread, daemon=True)
    progress_t = threading.Thread(target=progress_thread, daemon=True)

    download_t.start()
    progress_t.start()

@app.post("/models/download")
async def download_model(model_size: str = Form(...)):
    """
    ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì²« ë¡œë“œ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œë¨)

    Args:
        model_size: ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large-v3)

    Returns:
        ë‹¤ìš´ë¡œë“œ ì‹œì‘ ë©”ì‹œì§€
    """
    global download_progress

    if not WHISPER_AVAILABLE:
        raise HTTPException(status_code=500, detail="Whisper not available")

    available_models = WhisperHandler.get_available_models()
    if model_size not in available_models:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid model size. Available: {list(available_models.keys())}"
        )

    # ì´ë¯¸ ë‹¤ìš´ë¡œë“œ ì¤‘ì¸ì§€ í™•ì¸
    if model_size in download_progress:
        return {
            "status": "already_downloading",
            "message": f"Model {model_size} is already being downloaded",
            "model_size": model_size
        }

    try:
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œì‘
        await download_model_background(model_size)

        return {
            "status": "started",
            "message": f"Model {model_size} download started",
            "model_size": model_size
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error starting download: {str(e)}")

@app.delete("/models/{model_size}")
async def delete_model(model_size: str):
    """
    ëª¨ë¸ ì‚­ì œ

    Args:
        model_size: ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large-v3)

    Returns:
        ì‚­ì œ ì„±ê³µ/ì‹¤íŒ¨ ë©”ì‹œì§€
    """
    if not WHISPER_AVAILABLE:
        raise HTTPException(status_code=500, detail="Whisper not available")

    available_models = WhisperHandler.get_available_models()
    if model_size not in available_models:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid model size. Available: {list(available_models.keys())}"
        )

    try:
        import shutil
        from pathlib import Path

        # Hugging Face cache ê²½ë¡œ
        cache_dir = Path.home() / ".cache" / "huggingface" / "hub"

        # faster-whisper ëª¨ë¸ ì´ë¦„ í˜•ì‹: models--Systran--faster-whisper-{model_size}
        model_dir_name = f"models--Systran--faster-whisper-{model_size}"
        model_path = cache_dir / model_dir_name

        if model_path.exists():
            shutil.rmtree(model_path)
            print(f"Deleted model directory: {model_path}")

            return {
                "status": "success",
                "message": f"Model {model_size} deleted successfully",
                "model_size": model_size
            }
        else:
            return {
                "status": "success",
                "message": f"Model {model_size} was not downloaded",
                "model_size": model_size
            }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting model: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
